#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå¯¦éŸ³è¨Šæª”æ¡ˆæ¸¬è©¦ Omnilingual ASRï¼ˆæ”¯æ´é•·éŸ³è¨Šåˆ†æ®µè™•ç†ï¼‰
æ¸¬è©¦æª”æ¡ˆï¼šä»€éº¼æ˜¯ä¸Šå¸çš„é“.mp3
"""

import torch
import torchaudio
import time
from pathlib import Path
from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline


def load_audio(audio_path: str, target_sr: int = 16000):
    """è¼‰å…¥éŸ³è¨Šæª”æ¡ˆä¸¦é‡æ¡æ¨£åˆ°ç›®æ¨™æ¡æ¨£ç‡"""
    print(f"\nè¼‰å…¥éŸ³è¨Šæª”æ¡ˆ: {audio_path}")

    # è¼‰å…¥éŸ³è¨Š
    waveform, sample_rate = torchaudio.load(audio_path)

    # å¦‚æœæ˜¯ç«‹é«”è²ï¼Œè½‰ç‚ºå–®è²é“
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
        print(f"  å·²è½‰æ›ç‚ºå–®è²é“")

    # é‡æ¡æ¨£åˆ°ç›®æ¨™æ¡æ¨£ç‡
    if sample_rate != target_sr:
        resampler = torchaudio.transforms.Resample(sample_rate, target_sr)
        waveform = resampler(waveform)
        print(f"  å·²é‡æ¡æ¨£: {sample_rate} Hz -> {target_sr} Hz")

    # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦
    waveform = waveform.squeeze(0)

    duration = len(waveform) / target_sr
    print(f"  éŸ³è¨Šé•·åº¦: {duration:.2f} ç§’")
    print(f"  éŸ³è¨Šå½¢ç‹€: {waveform.shape}")

    return waveform, target_sr


def split_audio(waveform, sample_rate: int, chunk_duration: float = 30.0, overlap: float = 1.0):
    """
    å°‡é•·éŸ³è¨Šåˆ†æ®µè™•ç†

    Args:
        waveform: éŸ³è¨Šæ³¢å½¢
        sample_rate: æ¡æ¨£ç‡
        chunk_duration: æ¯æ®µé•·åº¦ï¼ˆç§’ï¼‰
        overlap: é‡ç–Šé•·åº¦ï¼ˆç§’ï¼‰ï¼Œé¿å…å¥å­è¢«åˆ‡æ–·

    Returns:
        chunks: åˆ†æ®µåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« (start_time, waveform_chunk)
    """
    total_samples = len(waveform)
    chunk_samples = int(chunk_duration * sample_rate)
    overlap_samples = int(overlap * sample_rate)
    step_samples = chunk_samples - overlap_samples

    chunks = []
    start_sample = 0

    while start_sample < total_samples:
        end_sample = min(start_sample + chunk_samples, total_samples)
        chunk = waveform[start_sample:end_sample]

        start_time = start_sample / sample_rate
        chunks.append((start_time, chunk))

        if end_sample >= total_samples:
            break

        start_sample += step_samples

    return chunks


def test_asr(
    audio_path: str,
    model_card: str = "omniASR_CTC_300M",
    device: str = "cuda",
    lang: str = "cmn_Hant",  # ä¸­æ–‡
    chunk_duration: float = 30.0,  # æ¯æ®µ 30 ç§’
    overlap: float = 1.0,  # é‡ç–Š 1 ç§’
):
    """æ¸¬è©¦ ASR ç³»çµ±ï¼ˆæ”¯æ´é•·éŸ³è¨Šï¼‰"""

    print("=" * 70)
    print("Omnilingual ASR é•·éŸ³è¨Šæ¸¬è©¦")
    print("=" * 70)

    # æª¢æŸ¥è£ç½®
    if device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œåˆ‡æ›åˆ° CPU æ¨¡å¼")
        device = "cpu"

    if device == "cuda":
        print(f"\nâœ“ ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print(f"\nâš ï¸  ä½¿ç”¨ CPU æ¨¡å¼")

    # è¼‰å…¥éŸ³è¨Š
    waveform, sample_rate = load_audio(audio_path)
    audio_duration = len(waveform) / sample_rate

    # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
    MAX_DURATION = 40.0  # æ¨¡å‹é™åˆ¶
    if audio_duration > MAX_DURATION:
        print(f"\nâš ï¸  éŸ³è¨Šé•·åº¦ ({audio_duration:.2f}s) è¶…éé™åˆ¶ ({MAX_DURATION}s)")
        print(f"  å°‡åˆ†æ®µè™•ç†ï¼šæ¯æ®µ {chunk_duration}sï¼Œé‡ç–Š {overlap}s")
        chunks = split_audio(waveform, sample_rate, chunk_duration, overlap)
        print(f"  å…±åˆ†ç‚º {len(chunks)} æ®µ")
    else:
        chunks = [(0, waveform)]
        print(f"\nâœ“ éŸ³è¨Šé•·åº¦åœ¨é™åˆ¶å…§ï¼Œç„¡éœ€åˆ†æ®µ")

    # è¼‰å…¥æ¨¡å‹
    print(f"\nè¼‰å…¥æ¨¡å‹: {model_card}")
    print(f"  ç›®æ¨™èªè¨€: {lang} (ä¸­æ–‡)")

    start_time = time.time()
    pipeline = ASRInferencePipeline(
        model_card=model_card,
        device=device,
        dtype=torch.float16 if device == "cuda" else torch.float32,
    )
    load_time = time.time() - start_time
    print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ ({load_time:.2f}s)")

    # åŸ·è¡Œè¾¨è­˜
    print("\n" + "=" * 70)
    print("é–‹å§‹èªéŸ³è¾¨è­˜...")
    print("=" * 70)

    all_results = []
    total_inference_time = 0

    for i, (start_time_sec, chunk) in enumerate(chunks, 1):
        print(f"\nè™•ç†ç¬¬ {i}/{len(chunks)} æ®µ (é–‹å§‹æ–¼ {start_time_sec:.1f}s)...")

        chunk_start = time.time()

        # æº–å‚™è¼¸å…¥
        inp = [{
            "waveform": chunk,
            "sample_rate": sample_rate,
        }]

        # åŸ·è¡Œè½‰éŒ„
        result = pipeline.transcribe(
            inp=inp,
            batch_size=1,
            lang=[lang],
        )

        chunk_time = time.time() - chunk_start
        total_inference_time += chunk_time

        chunk_duration_actual = len(chunk) / sample_rate
        rtf = chunk_time / chunk_duration_actual

        print(f"  âœ“ å®Œæˆ ({chunk_time:.2f}s, RTF: {rtf:.3f})")
        print(f"  æ–‡å­—: {result[0][:80]}...")

        all_results.append({
            "start_time": start_time_sec,
            "duration": chunk_duration_actual,
            "text": result[0],
            "inference_time": chunk_time,
        })

    # åˆä½µçµæœ
    full_transcription = "\n".join([r["text"] for r in all_results])

    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 70)
    print("å®Œæ•´è¾¨è­˜çµæœ")
    print("=" * 70)
    print(f"\n{full_transcription}\n")

    # æ•ˆèƒ½æŒ‡æ¨™
    print("=" * 70)
    print("æ•ˆèƒ½æŒ‡æ¨™")
    print("=" * 70)
    print(f"  éŸ³è¨Šç¸½é•·åº¦: {audio_duration:.2f} ç§’")
    print(f"  åˆ†æ®µæ•¸é‡: {len(chunks)}")
    print(f"  ç¸½è™•ç†æ™‚é–“: {total_inference_time:.2f} ç§’")
    print(f"  å¹³å‡ RTF: {total_inference_time / audio_duration:.3f}")
    print(f"  å¹³å‡é€Ÿåº¦: {audio_duration / total_inference_time:.2f}x å¯¦æ™‚é€Ÿåº¦")

    if device == "cuda":
        print(f"\nGPU è¨˜æ†¶é«”ä½¿ç”¨:")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
        print(f"  æœ€å¤§ä½¿ç”¨: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB")

    # å­—æ•¸çµ±è¨ˆ
    total_chars = len(full_transcription)
    total_words = len(full_transcription.split())
    print(f"\næ–‡å­—çµ±è¨ˆ:")
    print(f"  ç¸½å­—æ•¸: {total_chars}")
    print(f"  ç¸½è©æ•¸: {total_words}")

    print("\n" + "=" * 70)
    print("âœ“ æ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)

    return full_transcription, all_results


def main():
    """ä¸»ç¨‹å¼"""

    # éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
    audio_path = "/mnt/c/work/omnilingual-asr/ä»€éº¼æ˜¯ä¸Šå¸çš„é“.mp3"

    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not Path(audio_path).exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ: {audio_path}")
        return

    # æ¸¬è©¦åƒæ•¸
    config = {
        "audio_path": audio_path,
        "model_card": "omniASR_LLM_3B",  # CTC æ¨¡å‹: omniASR_CTC_300M, omniASR_CTC_1B, omniASR_CTC_3B, omniASR_CTC_7B
                                          # LLM æ¨¡å‹: omniASR_LLM_300M, omniASR_LLM_1B, omniASR_LLM_3B, omniASR_LLM_7B
                                          # âš ï¸ W2V æ¨¡å‹ (omniASR_W2V_*) ç„¡æ³•ç”¨æ–¼ ASR,åªç”¨æ–¼ SSL
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "lang": "cmn",  # ä¸­æ–‡ï¼ˆç¹é«”/ç°¡é«”é€šç”¨ï¼‰
        "chunk_duration": 30.0,  # æ¯æ®µ 30 ç§’
        "overlap": 1.0,  # é‡ç–Š 1 ç§’
    }

    print("\næ¸¬è©¦é…ç½®:")
    print(f"  éŸ³è¨Šæª”æ¡ˆ: {Path(audio_path).name}")
    print(f"  æ¨¡å‹: {config['model_card']}")
    print(f"  è£ç½®: {config['device']}")
    print(f"  èªè¨€: {config['lang']}")
    print(f"  åˆ†æ®µé•·åº¦: {config['chunk_duration']}s")
    print(f"  é‡ç–Šé•·åº¦: {config['overlap']}s")

    try:
        # åŸ·è¡Œæ¸¬è©¦
        full_text, segment_results = test_asr(**config)

        # å„²å­˜å®Œæ•´çµæœ
        output_file = "transcription_result.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"éŸ³è¨Šæª”æ¡ˆ: {Path(audio_path).name}\n")
            f.write(f"æ¨¡å‹: {config['model_card']}\n")
            f.write(f"èªè¨€: {config['lang']}\n")
            f.write(f"åˆ†æ®µæ•¸é‡: {len(segment_results)}\n")
            f.write(f"\n{'=' * 70}\n")
            f.write(f"å®Œæ•´è¾¨è­˜çµæœ:\n")
            f.write(f"{'=' * 70}\n\n")
            f.write(full_text)
            f.write(f"\n\n{'=' * 70}\n")
            f.write(f"åˆ†æ®µè©³ç´°çµæœ:\n")
            f.write(f"{'=' * 70}\n\n")

            for i, seg in enumerate(segment_results, 1):
                f.write(f"æ®µè½ {i} (é–‹å§‹æ–¼ {seg['start_time']:.1f}s):\n")
                f.write(f"{'-' * 70}\n")
                f.write(f"{seg['text']}\n\n")

        print(f"\nğŸ’¾ è¾¨è­˜çµæœå·²å„²å­˜åˆ°: {output_file}")

    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
